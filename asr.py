import os
import queue
from time import time

import numpy as np
import sounddevice as sd
from dotenv import load_dotenv
from faster_whisper import WhisperModel
from google import genai

import asyncio
import websockets
# ======================================================
# åŸºæœ¬è¨­å®š
# ======================================================

connected_clients = set()

async def ws_handler(websocket):
    connected_clients.add(websocket)
    try:
        await websocket.wait_closed()
    finally:
        connected_clients.remove(websocket)

async def start_ws_server():
    async with websockets.serve(ws_handler, "localhost", 8765):
        await asyncio.Future()  # run forever

async def broadcast_subtitle(text):
    if connected_clients:
        await asyncio.gather(
            *[client.send(text) for client in connected_clients]
        )


load_dotenv()

# Gemini clientï¼ˆæ–°ç‰ˆ SDKï¼‰
client = genai.Client(
    api_key=os.getenv("GEMINI_API_KEY")
)

# Whisper æ¨¡å‹ï¼ˆå³æ™‚å­—å¹•å»ºè­° smallï¼‰
whisper_model = WhisperModel(
    "small",
    device="cpu",        # æœ‰ GPU å¯æ”¹ "cuda"
    compute_type="int8"  # CPU çœè³‡æº
)

# Audio è¨­å®š
SAMPLE_RATE = 16000
CHUNK_SECONDS = 1.5
BLOCK_SIZE = int(SAMPLE_RATE * CHUNK_SECONDS)

audio_queue = queue.Queue()

# ======================================================
# Step 1ï¼šçŸ­å¥éæ¿¾
# ======================================================

MIN_KO_LENGTH = 6   # å°‘æ–¼ 6 å­—çš„éŸ“æ–‡ä¸ç¿»

# ======================================================
# Step 2ï¼šé‡è¤‡å¥å»é‡
# ======================================================

# last_ko_text = ""

# ======================================================
# Step 3ï¼šå¤šæ®µåˆä½µï¼ˆç¯€æµï¼‰
# ======================================================

# ko_buffer = []
# last_flush_time = time()
FLUSH_INTERVAL = 1.2

# ======================================================
# Step 4ï¼šç¿»è­¯çµæœå¿«å–
# ======================================================

# translation_cache = {}

# ======================================================
# Audio callback
# ======================================================

def audio_callback(indata, frames, time_info, status):
    if status:
        print("Audio status:", status)
    audio_queue.put(indata.copy())

# ======================================================
# Gemini ç¿»è­¯å‡½å¼
# ======================================================

def translate_ko_to_zh(text: str) -> str:
    prompt = f"""
ä½ æ˜¯ä¸€å€‹å³æ™‚ç›´æ’­å­—å¹•ç¿»è­¯å¼•æ“ã€‚
è«‹å°‡ä¸‹é¢çš„éŸ“æ–‡ç¿»è­¯æˆã€Œè‡ªç„¶ã€å£èªã€ç¹é«”ä¸­æ–‡ã€ã€‚
åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦è§£é‡‹ã€‚

éŸ“æ–‡ï¼š
{text}

ç¹é«”ä¸­æ–‡ï¼š
"""
    response = client.models.generate_content(
        model="models/gemini-2.5-flash",
        contents=prompt,
    )
    return response.text.strip()

# ======================================================
# ä¸»æµç¨‹
# ======================================================
def main():
    print("ğŸ§ é–‹å§‹ç›£è½è²éŸ³ï¼ˆCtrl+C çµæŸï¼‰...")

    ko_buffer = []
    last_flush_time = time()
    last_ko_text = ""
    translation_cache = {}

    with sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype="float32",
        blocksize=BLOCK_SIZE,
        callback=audio_callback,
    ):
        while True:
            audio_chunk = audio_queue.get()
            audio_chunk = audio_chunk.flatten()

            # Whisper ASR
            segments, _ = whisper_model.transcribe(
                audio_chunk,
                language="ko"
            )

            for seg in segments:
                ko_text = seg.text.strip()
                if not ko_text:
                    continue

                # --------------------------------------------------
                # Step 1ï¼šçŸ­å¥éæ¿¾
                # --------------------------------------------------
                if len(ko_text) < MIN_KO_LENGTH:
                    continue

                # --------------------------------------------------
                # Step 2ï¼šé‡è¤‡å¥å»é‡
                # --------------------------------------------------
                if ko_text == last_ko_text:
                    continue
                last_ko_text = ko_text

                # --------------------------------------------------
                # Step 3ï¼šç´¯ç©åˆ° buffer
                # --------------------------------------------------
                ko_buffer.append(ko_text)

            # --------------------------------------------------
            # Step 3ï¼šå®šæ™‚ flush buffer
            # --------------------------------------------------
            now = time()
            if ko_buffer and (now - last_flush_time >= FLUSH_INTERVAL):
                merged_text = " ".join(ko_buffer)
                ko_buffer.clear()
                last_flush_time = now

                if len(merged_text) < MIN_KO_LENGTH:
                    continue

                print(f"ğŸ‡°ğŸ‡· {merged_text}")

                # --------------------------------------------------
                # Step 4ï¼šç¿»è­¯å¿«å–
                # --------------------------------------------------
                if merged_text in translation_cache:
                    zh_text = translation_cache[merged_text]
                else:
                    try:
                        zh_text = translate_ko_to_zh(merged_text)
                        translation_cache[merged_text] = zh_text
                    except Exception as e:
                        print("âš ï¸ ç¿»è­¯å¤±æ•—ï¼š", e)
                        continue

                print(f"ğŸ‡¹ğŸ‡¼ {zh_text}\n")
                asyncio.run(broadcast_subtitle(zh_text))

if __name__ == "__main__":
    import threading

    # å•Ÿå‹• WebSocket Serverï¼ˆèƒŒæ™¯ï¼‰
    threading.Thread(
        target=lambda: asyncio.run(start_ws_server()),
        daemon=True
    ).start()

    # å•Ÿå‹•ä¸»æµç¨‹
    main()